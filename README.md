# nanoGPT

A minimal, educational implementation of a Generative Pretrained Transformer (GPT) model in Jupyter Notebook format.

## Overview

This repository demonstrates a simple and clear implementation of a GPT-like language model using Python and Jupyter Notebook. The goal is to help learners and practitioners understand the core concepts behind transformer-based language models by providing a concise and easily modifiable codebase.

The code and approach are inspired by the excellent educational content from [Andrej Karpathy's YouTube series](https://www.youtube.com/watch?v=kCc8FmEb1nY), which walks through the process of building a GPT model from scratch.

## Features

- End-to-end implementation of a transformer-based language model in Jupyter Notebook.
- Focus on readability and educational value over performance or scalability.
- Modular code structure for easy experimentation and extension.
- Suitable for learning, prototyping, and experimentation with transformer architectures.


## Credits

- Inspired by the educational series ["Let's build GPT: from scratch, in code, spelled out."](https://www.youtube.com/watch?v=kCc8FmEb1nY) by [Andrej Karpathy](https://www.youtube.com/@AndrejKarpathy).
- Original code and concepts adapted for clarity and educational use.

## License

This project is for educational purposes. Please check individual notebook and source code files for additional licensing or attribution if present.

---

Happy learning and experimenting!
