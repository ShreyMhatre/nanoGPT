{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNy2AotN0H0TOmBPdhgxqU1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyMhatre/nanoGPT/blob/main/nanoGPT_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`nanoGPT`**"
      ],
      "metadata": {
        "id": "XhhF7DhM-zm2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoanqt2s9Uvx",
        "outputId": "8cf60dc5-99c2-46d0-e706-1d9acc3735a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1-3910379859.py:6: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n",
            "  df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/thedevastator/tinystories-narrative-classification?dataset_version_number=2&file_name=validation.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.74M/5.74M [00:00<00:00, 32.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting zip of validation.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "file_path = \"validation.csv\"\n",
        "\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"thedevastator/tinystories-narrative-classification\",\n",
        "  file_path,\n",
        ")\n",
        "\n",
        "df.to_csv('input.txt', header=False, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "Lrrvbgm0A1KB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "b0cOM-O1A4qf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters & Config"
      ],
      "metadata": {
        "id": "Ch0Fhu5s_Sna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64   # Number of sequences per batch\n",
        "block_size = 512  # Maximum context length (tokens)\n",
        "max_iters = 5000  # Total training iterations\n",
        "eval_interval = 100 # Evaluate every N steps\n",
        "learning_rate = 1e-3 # Initial learning rate\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Use GPU if available\n",
        "eval_iters = 200 # Number of batches for evaluation\n",
        "n_embd = 192  # Embedding dimension\n",
        "n_head = 8  # Number of attention heads\n",
        "n_layer = 8  # Number of transformer blocks\n",
        "dropout = 0.2  # Dropout rate for regularization\n",
        "\n",
        "# Enable FlashAttention if available\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.enable_flash_sdp(True)\n",
        "\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVUS_oXE_TDa",
        "outputId": "e1d68a31-b15d-4c38-f5f2-78bc340fd88b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7da233d367d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading and Preprocessing"
      ],
      "metadata": {
        "id": "xSKL5uJdBOdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# Simple character-level tokenizer (can be replaced with BPE/WordPiece)\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)} # char -> index\n",
        "itos = {i: ch for i, ch in enumerate(chars)} # index -> char\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Convert full text to tensor of token indices\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# 90-10 train/validation split\n",
        "n = int(0.9 * len(data))\n",
        "train_data, val_data = data[:n], data[n:]"
      ],
      "metadata": {
        "id": "C02FPFIp_gfz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Batching Utilities"
      ],
      "metadata": {
        "id": "32M8_9tUBZdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix])         # Input sequence\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix]) # Target sequence\n",
        "    return x.to(device), y.to(device)"
      ],
      "metadata": {
        "id": "nJY09TQeBaJq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Utility"
      ],
      "metadata": {
        "id": "HwgkU4EkBcVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "MCfv2QJBBdVv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Components"
      ],
      "metadata": {
        "id": "eRpP8dEEBf5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single attention head with masking for causal (autoregressive) behavior\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=True)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=True)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=True)\n",
        "        self.register_buffer('mask', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k, q, v = self.key(x), self.query(x), self.value(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # Scaled dot-product attention\n",
        "        wei = wei.masked_fill(self.mask[:T, :T] == 0, float('-inf'))  # Causal masking\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        return wei @ v\n",
        "\n",
        "# Multi-head attention: runs several attention heads in parallel\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return self.dropout(self.proj(out))\n",
        "\n",
        "# Feedforward MLP with GELU activation\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Transformer Block: self-attention + feedforward + layer norm\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x)) # Residual + LayerNorm + MHA\n",
        "        return x + self.ffwd(self.ln2(x)) # Residual + LayerNorm + FFN"
      ],
      "metadata": {
        "id": "UFVZxVCbBlDt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Language Model Definition"
      ],
      "metadata": {
        "id": "Uf4BP7WTBqBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # Final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) # Output logits\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = self.blocks(tok_emb + pos_emb)\n",
        "        logits = self.lm_head(self.ln_f(x))\n",
        "        loss = F.cross_entropy(logits.view(B * T, -1), targets.view(B * T)) if targets is not None else None\n",
        "        return logits, loss\n",
        "\n",
        "    # Autoregressive generation\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(idx[:, -block_size:])\n",
        "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "iA3aNZHlBqpJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Initialization & Optimizer"
      ],
      "metadata": {
        "id": "yeHm-N0sBuum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel().to(device)\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, fused=True)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_iters)\n",
        "scaler = torch.amp.GradScaler(\"cuda\") # For mixed precision training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_GHJlteBvkD",
        "outputId": "53ae877a-4cea-4610-8147-fadc3a0290d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.696485 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "Uy3cF8igB4pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step {iter}: Train loss {losses['train']:.4f}, Val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "        logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        },
        "id": "TKqmU1a3B6jA",
        "outputId": "e0ebcf35-cf60-4f67-ad64-027804c233e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Train loss 5.3065, Val loss 5.3023\n",
            "Step 100: Train loss 2.3398, Val loss 2.3348\n",
            "Step 200: Train loss 2.2984, Val loss 2.2932\n",
            "Step 300: Train loss 2.2731, Val loss 2.2695\n",
            "Step 400: Train loss 2.2134, Val loss 2.2110\n",
            "Step 500: Train loss 2.0708, Val loss 2.0756\n",
            "Step 600: Train loss 1.8602, Val loss 1.8750\n",
            "Step 700: Train loss 1.6904, Val loss 1.7064\n",
            "Step 800: Train loss 1.5450, Val loss 1.5663\n",
            "Step 900: Train loss 1.4406, Val loss 1.4628\n",
            "Step 1000: Train loss 1.3435, Val loss 1.3640\n",
            "Step 1100: Train loss 1.2780, Val loss 1.2995\n",
            "Step 1200: Train loss 1.2201, Val loss 1.2463\n",
            "Step 1300: Train loss 1.1762, Val loss 1.1997\n",
            "Step 1400: Train loss 1.1356, Val loss 1.1608\n",
            "Step 1500: Train loss 1.1065, Val loss 1.1312\n",
            "Step 1600: Train loss 1.0827, Val loss 1.1020\n",
            "Step 1700: Train loss 1.0566, Val loss 1.0779\n",
            "Step 1800: Train loss 1.0344, Val loss 1.0589\n",
            "Step 1900: Train loss 1.0139, Val loss 1.0408\n",
            "Step 2000: Train loss 1.0031, Val loss 1.0271\n",
            "Step 2100: Train loss 0.9855, Val loss 1.0099\n",
            "Step 2200: Train loss 0.9714, Val loss 0.9968\n",
            "Step 2300: Train loss 0.9623, Val loss 0.9861\n",
            "Step 2400: Train loss 0.9481, Val loss 0.9726\n",
            "Step 2500: Train loss 0.9379, Val loss 0.9621\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-2231467252.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Generation"
      ],
      "metadata": {
        "id": "cKte8SU-B6y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_tokens = model.generate(context, max_new_tokens=1000)\n",
        "generated_text = decode(generated_tokens[0].tolist())\n",
        "\n",
        "print(\"\\nGenerated Text:\\n\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhU5DvN4B7RO",
        "outputId": "285fb0fc-2469-48b4-a43b-8a2c0d13a02f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:\n",
            " \n",
            "One day, a little girl named All named Lucy happened her called the coves were to make drumses and had but fly, had never seen bed seen. One day, Lily played to the box, the door into an a little girl was all hor hair. Lily was a big carrot, but she asked because if her nerguerer. From then day went up, hennigs playing in the widge of every colow.\"\n",
            "\"Once upon a time, there was a grow. It was a bead tire, Lily day. Buzzy said her friend continued to keep for sill around her love. They went back to the seal flow something and ally to.\n",
            "One day, Lily went to deep it the shelf appa on the sky and wanted the sned asked with the sunshine. \n",
            "\n",
            "Anbobow it, Billy said, \"\"The undover think youm, mom an+e with mine, ged with bread. A sweep fawing messs with her. With had teelep!\"\n",
            "\n",
            "Lily said thank the hugson the hill had not reall with the barage and replixed on the growadory together. Everyone with morning she look at her was tired. Sarah ran to the to play 1ty with there around tried.\n",
            "\n",
            "Sara followe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19ef29dd"
      },
      "source": [
        "torch.save(model.state_dict(), 'model.pth')"
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}